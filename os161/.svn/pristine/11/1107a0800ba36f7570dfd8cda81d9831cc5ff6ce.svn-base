#include <types.h>
#include <kern/errno.h>
#include <lib.h>
#include <thread.h>
#include <curthread.h>
#include <addrspace.h>
#include <vm.h>
#include <machine/spl.h>
#include <machine/tlb.h>

/*
 * Dumb MIPS-only "VM system" that is intended to only be just barely
 * enough to struggle off the ground. You should replace all of this
 * code while doing the VM assignment. In fact, starting in that
 * assignment, this file is not included in your kernel!
 */

/* under dumbvm, always have 48k of user stack */
#define DUMBVM_STACKPAGES    12

static int beforeVM = 1;
static int total_allocated_pages = 0;

static void print_coremap(void) {
	int i;

	// kprintf("\n");

	for (i = 0; i < coremap_size; i++) {
		kprintf("%d", coremap[i].allocated);
	}

	kprintf("\n total allocated page is %d\n", total_allocated_pages);
}

void
vm_bootstrap(void)
{
	paddr_t firstaddr, lastaddr;

	//firstaddr = ram_stealmem(0);
	//lastaddr = mips_ramsize();
	ram_getsize(&firstaddr, &lastaddr);

	//total - first page ---- includes coremap's page
	coremap_size = (lastaddr - firstaddr) / PAGE_SIZE;

	long size = sizeof(struct coremap_entry);

	coremap = (struct coremap_entry*)PADDR_TO_KVADDR(firstaddr);

	int i;
	for (i = 0; i < coremap_size; i++) {
		//increment the address
		//coremap[i].paddr = i * PAGE_SIZE;
		coremap[i].paddr = i * PAGE_SIZE + firstaddr;

		//whether or not the page has been allocated
		coremap[i].allocated = 0;
		coremap[i].kern_page = 0;

		//store the actual virtual address ------ may not use
		//coremap[i].vaddr = PADDR_TO_KVADDR(coremap[i].paddr);
	}

	coremap[0].allocated = 1;

	// print_coremap();

	beforeVM = 0;
	total_allocated_pages = 0;
}

//static
paddr_t
getppages(unsigned long npages)
{
	paddr_t addr;

	if (beforeVM) {
		int spl;

		spl = splhigh();

		addr = ram_stealmem(npages);
				
		splx(spl);
		return addr;
	} else {
		int spl;
		spl = splhigh();
		int i;
		int count_flag = 0, count = 0, num;

		for (i = 0; i <= coremap_size - npages; i++) { //1-6, so get 5,6
			if (!(coremap[i].allocated)) {
				for (num = 0; num < npages; num++) {
					if (!(coremap[i+num].allocated)) {		//check each consecutive one
						count++;
					} else {			//restart
						count = 0;
						break;
					}
				}
			}

			if (count == npages) {		//success
				count_flag = 1;
				break;
			} //else just continue to the next one
		}

		if (!count_flag) {
			splx(spl);
			return 0;
		}
		
		coremap[i].last_page = 1; // not currently used
		coremap[i].block_size = npages;

		int start = i;
		int index;
		for (i = 0; i < npages; i++) {
			index = start + i;
			coremap[index].kern_page = 0;
	        coremap[index].allocated = 1;
			coremap[index].vaddr = PADDR_TO_KVADDR(coremap[index].paddr);				//not sure about this
			total_allocated_pages++;

			//kprintf("\n      #pages     %d     allocated     %x      \n", npages, coremap[index].vaddr);
		}

		// kprintf("\n   pages  %d        allocated at  %x       total allocated size is  %d   \n", npages, coremap[start].vaddr, total_allocated_pages);
		// kprintf("\n      total allocated size is  %d         \n", total_allocated_pages);

		// print_coremap();

		splx(spl);
		addr = coremap[start].paddr;

		return addr;
	}

	return addr;
}

/* Allocate/free some kernel-space virtual pages 
	needs to be continuous*/
vaddr_t 
alloc_kpages(int npages)
{

	//kprintf("\n    in alloc     \n");

	// ORIGINAL CODE FROM OS161
	/*paddr_t pa;
	pa = getppages(npages);
	if (pa==0) {
		return 0;
	}

	return PADDR_TO_KVADDR(pa);
*/	
	int spl;
	spl = splhigh();
	int i;
	int count_flag = 0, count = 0, num;

	for (i = 0; i <= coremap_size - npages; i++) { //1-6, so get 5,6
		if (!(coremap[i].allocated)) {
			for (num = 0; num < npages; num++) {
				if (!(coremap[i+num].allocated)) {		//check each consecutive one
					count++;
				} else {			//restart
					count = 0;
					break;
				}
			}
		}

		if (count == npages) {		//success
			count_flag = 1;
			break;
		} //else just continue to the next one
	}

	if (!count_flag) {
		splx(spl);
		return 0;
	}
	
	coremap[i].last_page = 1;
	coremap[i].block_size = npages;

	int start = i;
	int index;
	for (i = 0; i < npages; i++) {
		index = start + i;
		coremap[index].kern_page = 1;
        coremap[index].allocated = 1;
		coremap[index].vaddr = PADDR_TO_KVADDR(coremap[index].paddr);				//not sure about this
		total_allocated_pages++;

		//kprintf("\n      #pages     %d     allocated     %x      \n", npages, coremap[index].vaddr);
	}

	// kprintf("\n   pages  %d        allocated at  %x       total allocated size is  %d   \n", npages, coremap[start].vaddr, total_allocated_pages);
	// kprintf("\n      total allocated size is  %d         \n", total_allocated_pages);

	// print_coremap();

	splx(spl);
	return coremap[start].vaddr;

	/*int spl;
	spl = splhigh();
	int i;
	int count_flag = 0, count = 0, num = 0;

	for (i = 1; i <= coremap_size-npages; i++) {
		if (!coremap[i].allocated) {
			for (num = 0; num < npages; num++) {
				if (!coremap[i+num].allocated) {		//check each consecutive one
					count++;
				} else {			//restart
					count = 0;
					break;
				}
			}
		}

		if (count == npages) {		//success
			count_flag = 1;
			break;
		} //else just continue to the next one
	}

	if (!count_flag) {
		splx(spl);
		return 0;
	}

	coremap[i].last_page = 1;

	int start = i;
	int index;
	for (i = 0; i < npages; i++) {
		index = start + i;
		coremap[index].kern_page = 1;
		coremap[index].allocated = 1;
		coremap[index].vaddr = PADDR_TO_KVADDR(coremap[index].paddr);				//not sure about this
	}

	splx(spl);
	return PADDR_TO_KVADDR(coremap[start].paddr);*/
}

void 
free_kpages(vaddr_t addr)
{
	//kprintf("\n     given address is  %x      \n", addr);

	int i;
	for (i = 0; i < coremap_size; i++) {
		if (coremap[i].vaddr == addr) {
			break;
		}
	}

	int size = coremap[i].block_size;
	
	int j;
	for (j = 0; j < size; j++) {
		int index = j + i;

		coremap[index].allocated = 0;
		coremap[index].last_page = 0;
		coremap[index].kern_page = 0;
		coremap[index].block_size = 0;
		total_allocated_pages--;

		//kprintf("\n      #pages     %d     freed     %x      \n", size, coremap[index].vaddr);
	}

	//print_coremap();

	// kprintf("\n   pages  %d        freed at  %x       total allocated size is  %d   \n", size, addr, total_allocated_pages);
	// kprintf("\n      total allocated size is  %d         \n", total_allocated_pages);
}


/* VM fault need to do list:
1. Copy text and data contents from old to new
2. Copy page table contents from old t
3. Create
 */

int
vm_fault(int faulttype, vaddr_t faultaddress) {
    //	vaddr_t vbase1, vtop1, vbase2, vtop2, stackbase, stacktop;
    vaddr_t vbase_stack, vtop_stack, vbase_heap, vtop_heap, vbase_data, vtop_data, vbase_text, vtop_text;
    paddr_t paddr, intermediate;
    int i;
    u_int32_t ehi, elo;
    struct addrspace *as;
    int spl;

    spl = splhigh();

    faultaddress &= PAGE_FRAME; //already masked for our lazy asses

    DEBUG(DB_VM, "dumbvm: fault: 0x%x\n", faultaddress);

    switch (faulttype) {
        case VM_FAULT_READONLY:
            /* We always create pages read-write, so we can't get this */
            panic("dumbvm: got VM_FAULT_READONLY\n");
        case VM_FAULT_READ:
        case VM_FAULT_WRITE:
            break;
        default:
            splx(spl);
            return EINVAL;
    }

    as = curthread->t_vmspace;
    if (as == NULL) {
        /*
         * No address space set up. This is probably a kernel
         * fault early in boot. Return EFAULT so as to panic
         * instead of getting into an infinite faulting loop.
         */
        return EFAULT;
    }

    /* Assert that the address space has been set up properly. */
    //	assert(as->as_vbase1 != 0);
    //	assert(as->as_pbase1 != 0);
    //	assert(as->as_npages1 != 0);
    //	assert(as->as_vbase2 != 0);
    //	assert(as->as_pbase2 != 0);
    //	assert(as->as_npages2 != 0);
    //	assert(as->as_stackpbase != 0);
    //	assert((as->as_vbase1 & PAGE_FRAME) == as->as_vbase1);
    //	assert((as->as_pbase1 & PAGE_FRAME) == as->as_pbase1);
    //	assert((as->as_vbase2 & PAGE_FRAME) == as->as_vbase2);
    //	assert((as->as_pbase2 & PAGE_FRAME) == as->as_pbase2);
    //	assert((as->as_stackpbase & PAGE_FRAME) == as->as_stackpbase);

    //make sure the address space has been properly set up
    // 


    vbase_stack = as->stack.region_start;
    vtop_stack = as->stack.region_end;
    vbase_heap = as->heap.region_start;
    vtop_heap = as->heap.region_end;
    vbase_data = as->data.region_start;
    vtop_data = as->data.region_end;
    vbase_text = as->text.region_start;
    vtop_text = as->text.region_end;


    //check for page table to see if previously loaded
    //if there, return the physical address
    //else, need to load_elf

    if (faultaddress >= vbase_data && faultaddress < vtop_data) {
        paddr = find_paddr(as, faultaddress);

    } else if (faultaddress >= vbase_text && faultaddress < vtop_text) {
        paddr = find_paddr(as, faultaddress);

    } else if (faultaddress >= vbase_stack && faultaddress < vtop_stack) {
        paddr = find_paddr(as, faultaddress);
        
    } else if (faultaddress >= vbase_heap && faultaddress < vtop_heap) {
        paddr = find_paddr(as, faultaddress);
        
    } else {
        
        splx(spl);
        return EFAULT;
    }

    


    //	vbase1 = as->as_vbase1;
    //	vtop1 = vbase1 + as->as_npages1 * PAGE_SIZE;
    //	vbase2 = as->as_vbase2;
    //	vtop2 = vbase2 + as->as_npages2 * PAGE_SIZE;
    //	stackbase = USERSTACK - DUMBVM_STACKPAGES * PAGE_SIZE;
    //	stacktop = USERSTACK;

    //	if (faultaddress >= vbase1 && faultaddress < vtop1) {
    //		paddr = (faultaddress - vbase1) + as->as_pbase1;
    //	}
    //	else if (faultaddress >= vbase2 && faultaddress < vtop2) {
    //		paddr = (faultaddress - vbase2) + as->as_pbase2;
    //	}
    //	else if (faultaddress >= stackbase && faultaddress < stacktop) {
    //		paddr = (faultaddress - stackbase) + as->as_stackpbase;
    //	}
    //	else {
    //		splx(spl);
    //		return EFAULT;
    //	}

    /* make sure it's page-aligned */
    //write the contents to TLB
    assert((paddr & PAGE_FRAME) == paddr);

    for (i = 0; i < NUM_TLB; i++) {
        TLB_Read(&ehi, &elo, i);
        if (elo & TLBLO_VALID) {
            continue;
        }
        ehi = faultaddress;
        elo = paddr | TLBLO_DIRTY | TLBLO_VALID;
        DEBUG(DB_VM, "dumbvm: 0x%x -> 0x%x\n", faultaddress, paddr);
        TLB_Write(ehi, elo, i);
        splx(spl);
        return 0;
    }

    kprintf("dumbvm: Ran out of TLB entries - cannot handle page fault\n");
    splx(spl);
    return EFAULT;
}









// int
// vm_fault(int faulttype, vaddr_t faultaddress)
// {
// 	vaddr_t vbase1, vtop1, vbase2, vtop2, stackbase, stacktop;
// 	paddr_t paddr;
// 	int i;
// 	u_int32_t ehi, elo;
// 	struct addrspace *as;
// 	int spl;

// 	spl = splhigh();

// 	faultaddress &= PAGE_FRAME;

// 	DEBUG(DB_VM, "dumbvm: fault: 0x%x\n", faultaddress);

// 	switch (faulttype) {
// 	    case VM_FAULT_READONLY:
// 		/* We always create pages read-write, so we can't get this */
// 		panic("dumbvm: got VM_FAULT_READONLY\n");
// 	    case VM_FAULT_READ:
// 	    case VM_FAULT_WRITE:
// 		break;
// 	    default:
// 		splx(spl);
// 		return EINVAL;
// 	}

// 	as = curthread->t_vmspace;
// 	if (as == NULL) {
// 		/*
// 		 * No address space set up. This is probably a kernel
// 		 * fault early in boot. Return EFAULT so as to panic
// 		 * instead of getting into an infinite faulting loop.
// 		 */
// 		return EFAULT;
// 	}

// 	/* Assert that the address space has been set up properly. */
// 	assert(as->as_vbase1 != 0);
// 	assert(as->as_pbase1 != 0);
// 	assert(as->as_npages1 != 0);
// 	assert(as->as_vbase2 != 0);
// 	assert(as->as_pbase2 != 0);
// 	assert(as->as_npages2 != 0);
// 	assert(as->as_stackpbase != 0);
// 	assert((as->as_vbase1 & PAGE_FRAME) == as->as_vbase1);
// 	assert((as->as_pbase1 & PAGE_FRAME) == as->as_pbase1);
// 	assert((as->as_vbase2 & PAGE_FRAME) == as->as_vbase2);
// 	assert((as->as_pbase2 & PAGE_FRAME) == as->as_pbase2);
// 	assert((as->as_stackpbase & PAGE_FRAME) == as->as_stackpbase);

// 	vbase1 = as->as_vbase1;
// 	vtop1 = vbase1 + as->as_npages1 * PAGE_SIZE;
// 	vbase2 = as->as_vbase2;
// 	vtop2 = vbase2 + as->as_npages2 * PAGE_SIZE;
// 	stackbase = USERSTACK - DUMBVM_STACKPAGES * PAGE_SIZE;
// 	stacktop = USERSTACK;

// 	if (faultaddress >= vbase1 && faultaddress < vtop1) {
// 		paddr = (faultaddress - vbase1) + as->as_pbase1;
// 	}
// 	else if (faultaddress >= vbase2 && faultaddress < vtop2) {
// 		paddr = (faultaddress - vbase2) + as->as_pbase2;
// 	}
// 	else if (faultaddress >= stackbase && faultaddress < stacktop) {
// 		paddr = (faultaddress - stackbase) + as->as_stackpbase;
// 	}
// 	else {
// 		splx(spl);
// 		return EFAULT;
// 	}

// 	/* make sure it's page-aligned */
// 	assert((paddr & PAGE_FRAME)==paddr);

// 	for (i=0; i<NUM_TLB; i++) {
// 		TLB_Read(&ehi, &elo, i);
// 		if (elo & TLBLO_VALID) {
// 			continue;
// 		}
// 		ehi = faultaddress;
// 		elo = paddr | TLBLO_DIRTY | TLBLO_VALID;
// 		DEBUG(DB_VM, "dumbvm: 0x%x -> 0x%x\n", faultaddress, paddr);
// 		TLB_Write(ehi, elo, i);
// 		splx(spl);
// 		return 0;
// 	}

// 	kprintf("dumbvm: Ran out of TLB entries - cannot handle page fault\n");
// 	splx(spl);
// 	return EFAULT;
// }




/* 		NOT USED ------ LOOK AT ADDRSPACE.C      */



// struct addrspace *
// as_create(void)
// {
// 	struct addrspace *as = kmalloc(sizeof(struct addrspace));
// 	if (as==NULL) {
// 		return NULL;
// 	}

// 	as->as_vbase1 = 0;
// 	as->as_pbase1 = 0;
// 	as->as_npages1 = 0;
// 	as->as_vbase2 = 0;
// 	as->as_pbase2 = 0;
// 	as->as_npages2 = 0;
// 	as->as_stackpbase = 0;

// 	return as;
// }

// void
// as_destroy(struct addrspace *as)
// {
// 	kfree(as);
// }

// void
// as_activate(struct addrspace *as)
// {
// 	int i, spl;

// 	(void)as;

// 	spl = splhigh();

// 	for (i=0; i<NUM_TLB; i++) {
// 		TLB_Write(TLBHI_INVALID(i), TLBLO_INVALID(), i);
// 	}

// 	splx(spl);
// }

// int
// as_define_region(struct addrspace *as, vaddr_t vaddr, size_t sz,
// 		 int readable, int writeable, int executable)
// {
// 	size_t npages; 

// 	/* Align the region. First, the base... */
// 	sz += vaddr & ~(vaddr_t)PAGE_FRAME;
// 	vaddr &= PAGE_FRAME;

// 	/* ...and now the length. */
// 	sz = (sz + PAGE_SIZE - 1) & PAGE_FRAME;

// 	npages = sz / PAGE_SIZE;

// 	/* We don't use these - all pages are read-write */
// 	(void)readable;
// 	(void)writeable;
// 	(void)executable;

// 	if (as->as_vbase1 == 0) {
// 		as->as_vbase1 = vaddr;
// 		as->as_npages1 = npages;
// 		return 0;
// 	}

// 	if (as->as_vbase2 == 0) {
// 		as->as_vbase2 = vaddr;
// 		as->as_npages2 = npages;
// 		return 0;
// 	}

// 	/*
// 	 * Support for more than two regions is not available.
// 	 */
// 	kprintf("dumbvm: Warning: too many regions\n");
// 	return EUNIMP;
// }

// int
// as_prepare_load(struct addrspace *as)
// {
// 	assert(as->as_pbase1 == 0);
// 	assert(as->as_pbase2 == 0);
// 	assert(as->as_stackpbase == 0);

// 	as->as_pbase1 = getppages(as->as_npages1);
// 	if (as->as_pbase1 == 0) {
// 		return ENOMEM;
// 	}

// 	as->as_pbase2 = getppages(as->as_npages2);
// 	if (as->as_pbase2 == 0) {
// 		return ENOMEM;
// 	}

// 	as->as_stackpbase = getppages(DUMBVM_STACKPAGES);
// 	if (as->as_stackpbase == 0) {
// 		return ENOMEM;
// 	}

// 	return 0;
// }

// int
// as_complete_load(struct addrspace *as)
// {
// 	(void)as;
// 	return 0;
// }

// int
// as_define_stack(struct addrspace *as, vaddr_t *stackptr)
// {
// 	assert(as->as_stackpbase != 0);

// 	*stackptr = USERSTACK;
// 	return 0;
// }

// int
// as_copy(struct addrspace *old, struct addrspace **ret)
// {
// 	struct addrspace *new;

// 	new = as_create();
// 	if (new==NULL) {
// 		return ENOMEM;
// 	}

// 	new->as_vbase1 = old->as_vbase1;
// 	new->as_npages1 = old->as_npages1;
// 	new->as_vbase2 = old->as_vbase2;
// 	new->as_npages2 = old->as_npages2;

// 	if (as_prepare_load(new)) {
// 		as_destroy(new);
// 		return ENOMEM;
// 	}

// 	assert(new->as_pbase1 != 0);
// 	assert(new->as_pbase2 != 0);
// 	assert(new->as_stackpbase != 0);

// 	memmove((void *)PADDR_TO_KVADDR(new->as_pbase1),
// 		(const void *)PADDR_TO_KVADDR(old->as_pbase1),
// 		old->as_npages1*PAGE_SIZE);

// 	memmove((void *)PADDR_TO_KVADDR(new->as_pbase2),
// 		(const void *)PADDR_TO_KVADDR(old->as_pbase2),
// 		old->as_npages2*PAGE_SIZE);

// 	memmove((void *)PADDR_TO_KVADDR(new->as_stackpbase),
// 		(const void *)PADDR_TO_KVADDR(old->as_stackpbase),
// 		DUMBVM_STACKPAGES*PAGE_SIZE);
	
// 	*ret = new;
// 	return 0;
// }